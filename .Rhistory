X <- runif(68500,0,180)
X = matrix(X,685,100)
y <- sample(1:3,685, replace=T) #labels
library("rjson")
data <- fromJSON(file = "final_data.json")
N <- unname(sapply(data[[1]],'[[',2))
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
library("rjson")
data <- fromJSON(file = "final_data.json")
N <- unname(sapply(data[[1]],'[[',2))
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
Y <- matrix(c(rep(c(0,0,1),N[1]),rep(c(0,1,0),N[2]),rep(c(1,0,0),N[3])), ncol = 3, byrow = TRUE)
# Y <- c(rep(0,N[1]),rep(1,N[2]),rep(2,N[3]))
# Pretreatment of the data
# [sigmamin,sigmamax] this is [0,1] for the logistic function
sigma_min <- 0
sigma_max <- 1
#Y_max <- max(Y)
#Y_min <- min(Y)
#Y_treat <- (sigma_max-sigma_min)/(Y_max-Y_min)*(Y-Y_min)+sigma_min
X_max <- apply(X,2,max)
X_min <- apply(X,2,min)
X_treat <- (sigma_max-sigma_min)/(X_max-X_min)*(X-X_min)+sigma_min
#intento2 <- c()
#for (i in 1:ncol(X)){
#  intento2 <- cbind(intento2, (sigma_max-sigma_min)/(X_max[i]-X_min[i])*(X[,i]-X_min[i])+sigma_min)
#}
#all.equal(X_treat,intento2)
X
X_treat
Y
X
N
data
dim(X)
zzz <- runif(68500,0,180)
zzz = matrix(zzz,685,100)
dim(zzz)
dim(Y)
Y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
# The goal of this exercise is creating a color recognition system
# for a data set using a one-vs-all logistic classifier.
# The colour pictures are characterized by a 10 x 10 pixel
# matrix that is stored in the rows. The labels
# corresponding to each digit is 1-blue 2-red 3-green
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
Y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
#X <- runif(68500,0,180)
# X = matrix(X,685,100)
#y <- sample(1:3,685, replace=T) #labels
n = dim(X)[1]
q = dim(X)[2]
# 4.- One-vs-all logistic regression: logistic classifier for the 3 colours.
num_labels = 3
beta_one_vs_all = matrix(0,q + 1, num_labels)
for (c in 1:num_labels) {
id_selected=which(y==c)
y_c = y
y_c[-id_selected] = 0
y_c[id_selected] = 1
data = data.frame(y_c,X)
model_glmfit_c = glm(y_c ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_c  = model_glmfit_c$coefficients # NA for linearly dependent vars
beta_glmfit_c[is.na(beta_glmfit_c)]=0
beta_one_vs_all[, c] = beta_glmfit_c
}
y_classified = apply( cbind(rep(1,n), X) %*% beta_one_vs_all , 1, FUN=which.max)
Empirical_error_one_vs_all = length(which(y_classified != y)) / n
# Confusion matrix
misclassification_matrix = matrix(0,num_labels, num_labels)
for (i in 1:num_labels) {
for (j in 1:num_labels) {
misclassification_matrix[i, j] = length(which((y == i) & (y_classified == j))) / length(which((y == i)))
}
}
# The goal of this exercise is creating a color recognition system
# for a data set using a one-vs-all logistic classifier.
# The colour pictures are characterized by a 10 x 10 pixel
# matrix that is stored in the rows. The labels
# corresponding to each digit is 1-blue 2-red 3-green
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
#X <- runif(68500,0,180)
# X = matrix(X,685,100)
#y <- sample(1:3,685, replace=T) #labels
n = dim(X)[1]
q = dim(X)[2]
# 4.- One-vs-all logistic regression: logistic classifier for the 3 colours.
num_labels = 3
beta_one_vs_all = matrix(0,q + 1, num_labels)
for (c in 1:num_labels) {
id_selected=which(y==c)
y_c = y
y_c[-id_selected] = 0
y_c[id_selected] = 1
data = data.frame(y_c,X)
model_glmfit_c = glm(y_c ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_c  = model_glmfit_c$coefficients # NA for linearly dependent vars
beta_glmfit_c[is.na(beta_glmfit_c)]=0
beta_one_vs_all[, c] = beta_glmfit_c
}
y_classified = apply( cbind(rep(1,n), X) %*% beta_one_vs_all , 1, FUN=which.max)
Empirical_error_one_vs_all = length(which(y_classified != y)) / n
# Confusion matrix
misclassification_matrix = matrix(0,num_labels, num_labels)
for (i in 1:num_labels) {
for (j in 1:num_labels) {
misclassification_matrix[i, j] = length(which((y == i) & (y_classified == j))) / length(which((y == i)))
}
}
data
# The goal of this exercise is creating a color recognition system
# for a data set using a one-vs-all logistic classifier.
# The colour pictures are characterized by a 10 x 10 pixel
# matrix that is stored in the rows. The labels
# corresponding to each digit is 1-blue 2-red 3-green
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
n = dim(X)[1]
q = dim(X)[2]
num_labels = 3
beta_one_vs_all = matrix(0,q + 1, num_labels)
for (c in 1:num_labels) {
id_selected=which(y==c)
y_c = y
y_c[-id_selected] = 0
y_c[id_selected] = 1
data = data.frame(y_c,X)
model_glmfit_c = glm(y_c ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_c  = model_glmfit_c$coefficients # NA for linearly dependent vars
beta_glmfit_c[is.na(beta_glmfit_c)]=0
beta_one_vs_all[, c] = beta_glmfit_c
}
y_classified = apply( cbind(rep(1,n), X) %*% beta_one_vs_all , 1, FUN=which.max)
Empirical_error_one_vs_all = length(which(y_classified != y)) / n
n = dim(X)[1]
q = dim(X)[2]
id_selected=which(y==1)
y_1 = y
y_1[-id_selected] = 0
data = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_1  = model_glmfit_1$coefficients # NA for linearly dependent vars
dev = model_glmfit_1$deviance
beta_glmfit_1[is.na(beta_glmfit_1)] = 0
etas_glmfit_1 = 1 / (1 + exp(- (cbind(rep(1,n), X) %*% beta_glmfit_1)  ))
TypeI_errors_glmfit_1 = length(which((etas_glmfit_1 > .5) & (y_1 != 1)))
TypeII_errors_glmfit_1 = length(which((etas_glmfit_1 <= .5) & (y_1 == 1)))
Empirical_error_glmfit_1 = (TypeI_errors_glmfit_1 + TypeII_errors_glmfit_1) / n
id_selected=which(y==2)
y_1 = y
y_1[-id_selected] = 0
data = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_1  = model_glmfit_1$coefficients # NA for linearly dependent vars
dev = model_glmfit_1$deviance
beta_glmfit_1[is.na(beta_glmfit_1)] = 0
etas_glmfit_1 = 1 / (1 + exp(- (cbind(rep(1,n), X) %*% beta_glmfit_1)  ))
TypeI_errors_glmfit_1 = length(which((etas_glmfit_1 > .5) & (y_1 != 1)))
TypeII_errors_glmfit_1 = length(which((etas_glmfit_1 <= .5) & (y_1 == 1)))
Empirical_error_glmfit_1 = (TypeI_errors_glmfit_1 + TypeII_errors_glmfit_1) / n
id_selected=which(y==3)
y_1 = y
y_1[-id_selected] = 0
data = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_1  = model_glmfit_1$coefficients # NA for linearly dependent vars
dev = model_glmfit_1$deviance
beta_glmfit_1[is.na(beta_glmfit_1)] = 0
etas_glmfit_1 = 1 / (1 + exp(- (cbind(rep(1,n), X) %*% beta_glmfit_1)  ))
TypeI_errors_glmfit_1 = length(which((etas_glmfit_1 > .5) & (y_1 != 1)))
TypeII_errors_glmfit_1 = length(which((etas_glmfit_1 <= .5) & (y_1 == 1)))
Empirical_error_glmfit_1 = (TypeI_errors_glmfit_1 + TypeII_errors_glmfit_1) / n
y
dim(y)
dim(y)[1]
lenght(y)
length(y)
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
n = dim(X)[1]
q = dim(X)[2]
id_selected=which(y==3)
id_selected
y_1 = y
y_1[-id_selected] = 0
y_1
data = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_1  = model_glmfit_1$coefficients # NA for linearly dependent vars
dev = model_glmfit_1$deviance
beta_glmfit_1[is.na(beta_glmfit_1)] = 0
data = data.frame(y_1,X)
# The goal of this exercise is creating a color recognition system
# for a data set using a one-vs-all logistic classifier.
# The colour pictures are characterized by a 10 x 10 pixel
# matrix that is stored in the rows. The labels
# corresponding to each digit is 1-blue 2-red 3-green
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
#X <- runif(68500,0,180)
# X = matrix(X,685,100)
#y <- sample(1:3,685, replace=T) #labels
n = dim(X)[1]
q = dim(X)[2]
id_selected=which(y==3)
y_1 = y
y_1[-id_selected] = 0
data_t = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data_t, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_1  = model_glmfit_1$coefficients # NA for linearly dependent vars
dev = model_glmfit_1$deviance
beta_glmfit_1[is.na(beta_glmfit_1)] = 0
etas_glmfit_1 = 1 / (1 + exp(- (cbind(rep(1,n), X) %*% beta_glmfit_1)  ))
TypeI_errors_glmfit_1 = length(which((etas_glmfit_1 > .5) & (y_1 != 3)))
TypeII_errors_glmfit_1 = length(which((etas_glmfit_1 <= .5) & (y_1 == 3)))
Empirical_error_glmfit_1 = (TypeI_errors_glmfit_1 + TypeII_errors_glmfit_1) / n
# The goal of this exercise is creating a color recognition system
# for a data set using a one-vs-all logistic classifier.
# The colour pictures are characterized by a 10 x 10 pixel
# matrix that is stored in the rows. The labels
# corresponding to each digit is 1-blue 2-red 3-green
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
#X <- runif(68500,0,180)
# X = matrix(X,685,100)
#y <- sample(1:3,685, replace=T) #labels
n = dim(X)[1]
q = dim(X)[2]
id_selected=which(y==3)
y_1 = y
y_1[-id_selected] = 0
data = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_1  = model_glmfit_1$coefficients # NA for linearly dependent vars
dev = model_glmfit_1$deviance
beta_glmfit_1[is.na(beta_glmfit_1)] = 0
etas_glmfit_1 = 1 / (1 + exp(- (cbind(rep(1,n), X) %*% beta_glmfit_1)  ))
TypeI_errors_glmfit_1 = length(which((etas_glmfit_1 > .5) & (y_1 != 3)))
TypeII_errors_glmfit_1 = length(which((etas_glmfit_1 <= .5) & (y_1 == 3)))
Empirical_error_glmfit_1 = (TypeI_errors_glmfit_1 + TypeII_errors_glmfit_1) / n
# The goal of this exercise is creating a color recognition system
# for a data set using a one-vs-all logistic classifier.
# The colour pictures are characterized by a 10 x 10 pixel
# matrix that is stored in the rows. The labels
# corresponding to each digit is 1-blue 2-red 3-green
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
#X <- runif(68500,0,180)
# X = matrix(X,685,100)
#y <- sample(1:3,685, replace=T) #labels
n = dim(X)[1]
q = dim(X)[2]
id_selected=which(y==1)
y_1 = y
y_1[-id_selected] = 0
data = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_1  = model_glmfit_1$coefficients # NA for linearly dependent vars
dev = model_glmfit_1$deviance
beta_glmfit_1[is.na(beta_glmfit_1)] = 0
etas_glmfit_1 = 1 / (1 + exp(- (cbind(rep(1,n), X) %*% beta_glmfit_1)  ))
TypeI_errors_glmfit_1 = length(which((etas_glmfit_1 > .5) & (y_1 != 1)))
TypeII_errors_glmfit_1 = length(which((etas_glmfit_1 <= .5) & (y_1 == 1)))
Empirical_error_glmfit_1 = (TypeI_errors_glmfit_1 + TypeII_errors_glmfit_1) / n
# The goal of this exercise is creating a color recognition system
# for a data set using a one-vs-all logistic classifier.
# The colour pictures are characterized by a 10 x 10 pixel
# matrix that is stored in the rows. The labels
# corresponding to each digit is 1-blue 2-red 3-green
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
#X <- runif(68500,0,180)
# X = matrix(X,685,100)
#y <- sample(1:3,685, replace=T) #labels
n = dim(X)[1]
q = dim(X)[2]
id_selected=which(y==2)
y_1 = y
y_1[-id_selected] = 0
data = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_1  = model_glmfit_1$coefficients # NA for linearly dependent vars
dev = model_glmfit_1$deviance
beta_glmfit_1[is.na(beta_glmfit_1)] = 0
etas_glmfit_1 = 1 / (1 + exp(- (cbind(rep(1,n), X) %*% beta_glmfit_1)  ))
TypeI_errors_glmfit_1 = length(which((etas_glmfit_1 > .5) & (y_1 != 2)))
TypeII_errors_glmfit_1 = length(which((etas_glmfit_1 <= .5) & (y_1 == 2)))
Empirical_error_glmfit_1 = (TypeI_errors_glmfit_1 + TypeII_errors_glmfit_1) / n
n = dim(X)[1]
q = dim(X)[2]
id_selected=which(y==2)
y_1 = y
y_1[-id_selected] = 0
data = data.frame(y_1,X)
model_glmfit_1 = glm(y_1 ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
# The goal of this exercise is creating a color recognition system
# for a data set using a one-vs-all logistic classifier.
# The colour pictures are characterized by a 10 x 10 pixel
# matrix that is stored in the rows. The labels
# corresponding to each digit is 1-blue 2-red 3-green
graphics.off() # close all plots
rm(list=ls()) # remove all objects from from the current workspace (R memory)
# Data
library("rjson")
data <- fromJSON(file = "final_data.json")
## DATA PROCESSING
# Extracting the number of blue, red and green samples.
# N[1]: blues, N[2]: reds, N[3]: greens
N <- unname(sapply(data[[1]],'[[',2))
# Generating the matrix samples X, vector Y (where 0: blue, 1: red, 2: green)
X <- c()
for (i in 1:length(data[[2]])){
X <- rbind(X, as.vector(apply(as.matrix(data[[2]][[i]][[4]]), 1, function(x) unlist(x)))) #ASK!!!! t(M) if by row
}
y <- c(rep(1,N[1]),rep(2,N[2]),rep(3,N[3]))
#X <- runif(68500,0,180)
# X = matrix(X,685,100)
#y <- sample(1:3,685, replace=T) #labels
n = dim(X)[1]
q = dim(X)[2]
num_labels = 3
beta_one_vs_all = matrix(0,q + 1, num_labels)
for (c in 1:num_labels) {
id_selected=which(y==c)
y_c = y
y_c[-id_selected] = 0
y_c[id_selected] = 1
data = data.frame(y_c,X)
model_glmfit_c = glm(y_c ~., data, start =rep(0,q+1) ,family=binomial(link="logit"),
control=list(maxit = 100, trace = FALSE) )
beta_glmfit_c  = model_glmfit_c$coefficients # NA for linearly dependent vars
beta_glmfit_c[is.na(beta_glmfit_c)]=0
beta_one_vs_all[, c] = beta_glmfit_c
}
y_classified = apply( cbind(rep(1,n), X) %*% beta_one_vs_all , 1, FUN=which.max)
y_classified
Empirical_error_one_vs_all = length(which(y_classified != y)) / n
misclassification_matrix = matrix(0,num_labels, num_labels)
for (i in 1:num_labels) {
for (j in 1:num_labels) {
misclassification_matrix[i, j] = length(which((y == i) & (y_classified == j))) / length(which((y == i)))
}
}
misclassification_matrix
